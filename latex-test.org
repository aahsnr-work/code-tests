#+TITLE: Complete Python Programming & Scientific Computing Guide
#+AUTHOR: Comprehensive Python Learning Resource
#+DATE: \today
#+EMAIL: your-email@example.com
#+STARTUP: latexpreview inlineimages
#+OPTIONS: toc:3 num:t H:5 ^:{} tags:nil
#+PROPERTY: header-args:python :session python_guide :results output :exports both
#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage{amsmath,physics,siunitx,booktabs,graphicx,listings}

* Document Metadata :noexport:
:PROPERTIES:
:CREATED:  [2025-11-01 Sat]
:CATEGORY: Python
:VERSION:  Python 3.13
:END:

This comprehensive guide covers all essential Python features (3.12/3.13), numerical computing, scientific computing, and machine learning.

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

#+begin_abstract
This document serves as a complete guide to Python programming, from fundamental concepts to advanced techniques in scientific computing and machine learning. We cover Python 3.13 features, including the experimental JIT compiler and free-threading mode, along with comprehensive coverage of essential libraries: NumPy, SciPy, Pandas, Matplotlib, Seaborn, Scikit-learn, TensorFlow, and PyTorch. This guide uses Org-mode's literate programming approach for reproducible, executable examples.
#+end_abstract

* Introduction to Python 3.13
:PROPERTIES:
:CUSTOM_ID: sec:introduction
:END:

** Why Python for Scientific Computing?

Python 3.13, released in October 2024, brings significant advancements including an experimental JIT compiler (PEP 744) with up to 30% speedups for computation-heavy tasks, experimental no-GIL mode for better parallelism (PEP 703), improved interactive REPL with syntax highlighting and color support, and a 7% smaller memory footprint compared to 3.12.

#+begin_src python :results silent
"""Welcome to Python 3.13 - Let's verify our environment"""
import sys
import platform

print(f"Python Version: {sys.version}")
print(f"Platform: {platform.system()} {platform.release()}")
print(f"Architecture: {platform.machine()}")
#+end_src

** Setting Up the Environment

#+begin_src python :results silent
"""Essential imports for scientific computing"""
import numpy as np
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Union, Any
from dataclasses import dataclass
from functools import wraps
import asyncio
from contextlib import contextmanager

# Set random seed for reproducibility
np.random.seed(42)

print("✓ All essential libraries loaded successfully!")
#+end_src

* Python Fundamentals
:PROPERTIES:
:CUSTOM_ID: sec:fundamentals
:END:

** Basic Data Types and Variables

#+begin_src python
"""Python's built-in data types"""

# Numbers: int, float, complex
integer = 42
floating = 3.14159
complex_num = 3 + 4j

print(f"Integer: {integer}, type: {type(integer)}")
print(f"Float: {floating}, type: {type(floating)}")
print(f"Complex: {complex_num}, type: {type(complex_num)}")

# Strings: immutable sequences
text = "Python 3.13"
multiline = """This is a
multiline string"""

# f-strings (formatted string literals) - Python 3.6+
name = "Python"
version = 3.13
print(f"{name} version {version:.1f}")

# Boolean
is_active = True
is_ready = False
#+end_src

** Data Structures

*** Lists: Mutable Sequences

#+begin_src python
"""Lists - ordered, mutable collections"""

# Creating lists
numbers = [1, 2, 3, 4, 5]
mixed = [1, "two", 3.0, [4, 5]]

# List comprehensions - Pythonic way to create lists
squares = [x**2 for x in range(10)]
evens = [x for x in range(20) if x % 2 == 0]

print(f"Squares: {squares}")
print(f"Evens: {evens}")

# Advanced: nested comprehensions
matrix = [[i*j for j in range(5)] for i in range(5)]
print(f"Matrix shape: {len(matrix)}x{len(matrix[0])}")

# List slicing
print(f"First 3: {numbers[:3]}")
print(f"Last 2: {numbers[-2:]}")
print(f"Reversed: {numbers[::-1]}")
#+end_src

*** Tuples: Immutable Sequences

#+begin_src python
"""Tuples - immutable, faster than lists"""

# Creating tuples
point = (10, 20)
rgb = (255, 128, 0)

# Tuple unpacking
x, y = point
r, g, b = rgb
print(f"Point: x={x}, y={y}")

# Named tuples (from collections)
from collections import namedtuple

Point = namedtuple('Point', ['x', 'y', 'z'])
p = Point(1, 2, 3)
print(f"3D Point: {p.x}, {p.y}, {p.z}")
#+end_src

*** Dictionaries: Key-Value Mappings

#+begin_src python
"""Dictionaries - hash maps with O(1) lookup"""

# Creating dictionaries
person = {
    'name': 'Alice',
    'age': 30,
    'city': 'New York'
}

# Dictionary comprehensions
squared_dict = {x: x**2 for x in range(6)}
print(f"Squared dict: {squared_dict}")

# Modern dict operations (Python 3.9+)
dict1 = {'a': 1, 'b': 2}
dict2 = {'c': 3, 'd': 4}
merged = dict1 | dict2  # Dictionary union operator
print(f"Merged: {merged}")

# Safe access with .get()
value = person.get('email', 'not provided')
print(f"Email: {value}")
#+end_src

*** Sets: Unique Collections

#+begin_src python
"""Sets - unordered collections of unique elements"""

# Creating sets
primes = {2, 3, 5, 7, 11}
evens = {2, 4, 6, 8, 10}

# Set operations
print(f"Union: {primes | evens}")
print(f"Intersection: {primes & evens}")
print(f"Difference: {primes - evens}")

# Set comprehensions
even_squares = {x**2 for x in range(10) if x % 2 == 0}
print(f"Even squares: {even_squares}")
#+end_src

** Control Flow

#+begin_src python
"""Control flow: if, for, while, match (Python 3.10+)"""

# If-elif-else
score = 85
if score >= 90:
    grade = 'A'
elif score >= 80:
    grade = 'B'
else:
    grade = 'C'
print(f"Grade: {grade}")

# For loops with enumerate and zip
fruits = ['apple', 'banana', 'cherry']
for idx, fruit in enumerate(fruits, start=1):
    print(f"{idx}. {fruit}")

prices = [1.0, 0.5, 2.0]
for fruit, price in zip(fruits, prices):
    print(f"{fruit}: ${price}")

# While loops
count = 0
while count < 3:
    print(f"Count: {count}")
    count += 1

# Pattern matching (Python 3.10+)
def describe_point(point):
    match point:
        case (0, 0):
            return "Origin"
        case (0, y):
            return f"Y-axis at {y}"
        case (x, 0):
            return f"X-axis at {x}"
        case (x, y):
            return f"Point at ({x}, {y})"

print(describe_point((0, 0)))
print(describe_point((3, 4)))
#+end_src

* Functions and Functional Programming
:PROPERTIES:
:CUSTOM_ID: sec:functions
:END:

** Function Basics

#+begin_src python
"""Functions: first-class objects in Python"""

# Basic function
def greet(name: str) -> str:
    """Simple greeting function with type hints"""
    return f"Hello, {name}!"

# Default arguments
def power(base: float, exponent: float = 2.0) -> float:
    """Calculate power with default exponent"""
    return base ** exponent

# Keyword arguments
def create_profile(name: str, *, age: int, city: str) -> Dict:
    """Force keyword-only arguments after *"""
    return {'name': name, 'age': age, 'city': city}

print(greet("Python"))
print(f"2^3 = {power(2, 3)}")
profile = create_profile("Alice", age=30, city="NYC")
print(profile)
#+end_src

** Lambda Functions

#+begin_src python
"""Lambda functions: anonymous functions"""

# Simple lambda
square = lambda x: x ** 2
print(f"square(5) = {square(5)}")

# Lambda with map, filter, reduce
numbers = [1, 2, 3, 4, 5]
squared = list(map(lambda x: x**2, numbers))
evens = list(filter(lambda x: x % 2 == 0, numbers))

from functools import reduce
product = reduce(lambda x, y: x * y, numbers)

print(f"Squared: {squared}")
print(f"Evens: {evens}")
print(f"Product: {product}")

# Sorting with lambda
people = [
    {'name': 'Alice', 'age': 30},
    {'name': 'Bob', 'age': 25},
    {'name': 'Charlie', 'age': 35}
]
sorted_by_age = sorted(people, key=lambda p: p['age'])
print(f"Sorted: {[p['name'] for p in sorted_by_age]}")
#+end_src

** Higher-Order Functions

#+begin_src python
"""Higher-order functions: functions that take/return functions"""

from typing import Callable

def apply_twice(func: Callable[[int], int], x: int) -> int:
    """Apply a function twice"""
    return func(func(x))

def make_multiplier(n: int) -> Callable[[int], int]:
    """Return a function that multiplies by n"""
    def multiplier(x: int) -> int:
        return x * n
    return multiplier

# Usage
result = apply_twice(lambda x: x + 1, 5)
print(f"Apply twice: {result}")  # (5+1)+1 = 7

times_three = make_multiplier(3)
print(f"5 * 3 = {times_three(5)}")
#+end_src

** Decorators (Advanced)
Decorators are a powerful Python feature that allows you to modify or enhance functions without changing their actual code, widely used for adding functionality like logging, timing, and caching.

#+begin_src python
"""Decorators: modify function behavior"""

import time
from functools import wraps

# Simple decorator
def timer(func):
    """Measure execution time"""
    @wraps(func)  # Preserve function metadata
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__} took {end - start:.4f}s")
        return result
    return wrapper

# Decorator with arguments
def repeat(times: int):
    """Repeat function execution"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for _ in range(times):
                result = func(*args, **kwargs)
            return result
        return wrapper
    return decorator

# Using decorators
@timer
@repeat(3)
def say_hello(name: str) -> str:
    print(f"Hello, {name}!")
    return name

say_hello("Python")
#+end_src

#+begin_src python
"""More decorator examples: memoization and logging"""

from functools import lru_cache
import logging

logging.basicConfig(level=logging.INFO)

# Built-in memoization decorator
@lru_cache(maxsize=128)
def fibonacci(n: int) -> int:
    """Fibonacci with automatic caching"""
    if n < 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Custom logging decorator
def log_calls(func):
    """Log function calls"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logging.info(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        result = func(*args, **kwargs)
        logging.info(f"{func.__name__} returned {result}")
        return result
    return wrapper

@log_calls
def add(a: int, b: int) -> int:
    return a + b

# Test
print(f"Fibonacci(10) = {fibonacci(10)}")
print(f"Add(3, 4) = {add(3, 4)}")
#+end_src

* Advanced Python Features
:PROPERTIES:
:CUSTOM_ID: sec:advanced
:END:

** Generators and Iterators
Generators are special iterables that compute elements on the fly using the yield keyword, making them incredibly memory-efficient for handling large datasets or infinite sequences.

#+begin_src python
"""Generators: memory-efficient iterators"""

# Simple generator
def countdown(n: int):
    """Count down from n to 0"""
    while n > 0:
        yield n
        n -= 1

# Generator expression
squares_gen = (x**2 for x in range(1000000))  # No memory allocated yet

# Infinite generator
def fibonacci_generator():
    """Infinite Fibonacci sequence"""
    a, b = 0, 1
    while True:
        yield a
        a, b = b, a + b

# Usage
print("Countdown:")
for num in countdown(5):
    print(num, end=' ')
print()

# Take first 10 Fibonacci numbers
from itertools import islice
fib_gen = fibonacci_generator()
first_10 = list(islice(fib_gen, 10))
print(f"First 10 Fibonacci: {first_10}")
#+end_src

#+begin_src python
"""Advanced generator patterns"""

# Generator with send()
def running_average():
    """Calculate running average"""
    total = 0.0
    count = 0
    average = None
    while True:
        value = yield average
        total += value
        count += 1
        average = total / count

# Pipeline pattern
def read_data():
    """Simulate reading data"""
    for i in range(10):
        yield i

def filter_even(numbers):
    """Filter even numbers"""
    for num in numbers:
        if num % 2 == 0:
            yield num

def square(numbers):
    """Square numbers"""
    for num in numbers:
        yield num ** 2

# Chain generators
pipeline = square(filter_even(read_data()))
result = list(pipeline)
print(f"Pipeline result: {result}")
#+end_src

** Context Managers
Context managers are mechanisms for managing resources like file handles, network connections, or locks, ensuring they are properly acquired and released using the with statement.

#+begin_src python
"""Context managers: resource management"""

# Using built-in context managers
with open('example.txt', 'w') as f:
    f.write("Hello, Python!")

# Creating context managers with __enter__ and __exit__
class Timer:
    """Context manager for timing code blocks"""
    
    def __enter__(self):
        self.start = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end = time.time()
        self.elapsed = self.end - self.start
        print(f"Elapsed time: {self.elapsed:.4f}s")
        return False  # Don't suppress exceptions

# Usage
with Timer():
    # Some time-consuming operation
    sum([i**2 for i in range(100000)])
#+end_src

#+begin_src python
"""Context managers with @contextmanager decorator"""

from contextlib import contextmanager

@contextmanager
def temporary_value(variable_dict: Dict, key: str, temp_value):
    """Temporarily change a dictionary value"""
    original = variable_dict.get(key)
    variable_dict[key] = temp_value
    try:
        yield variable_dict
    finally:
        if original is None:
            variable_dict.pop(key, None)
        else:
            variable_dict[key] = original

# Usage
config = {'debug': False, 'timeout': 30}
print(f"Original: {config}")

with temporary_value(config, 'debug', True):
    print(f"Inside context: {config}")

print(f"After context: {config}")
#+end_src

** Asynchronous Programming
Python's async and await keywords enable asynchronous programming, allowing efficient handling of I/O-bound operations and concurrent tasks without traditional threading complexity.

#+begin_src python
"""Async/await: asynchronous programming"""

import asyncio
from typing import List

# Basic async function
async def fetch_data(url: str, delay: float) -> Dict[str, Any]:
    """Simulate fetching data from a URL"""
    print(f"Fetching {url}...")
    await asyncio.sleep(delay)  # Simulate network delay
    return {'url': url, 'data': f'Data from {url}'}

# Async function that calls other async functions
async def fetch_multiple(urls: List[str]) -> List[Dict]:
    """Fetch multiple URLs concurrently"""
    tasks = [fetch_data(url, 0.5) for url in urls]
    results = await asyncio.gather(*tasks)
    return results

# Async generator
async def async_range(n: int):
    """Async generator example"""
    for i in range(n):
        await asyncio.sleep(0.1)
        yield i

# Running async code
async def main():
    """Main async function"""
    # Concurrent fetching
    urls = ['http://api1.com', 'http://api2.com', 'http://api3.com']
    results = await fetch_multiple(urls)
    print(f"Fetched {len(results)} URLs")
    
    # Async iteration
    print("Async range:")
    async for i in async_range(5):
        print(i, end=' ')
    print()

# Note: In Jupyter/IPython, use: await main()
# In scripts, use: asyncio.run(main())
print("Async example ready. Run with: asyncio.run(main())")
#+end_src

** Type Hints and Static Type Checking
Python 3.12 and 3.13 expanded the typing module with features like TypeIs, ReadOnly, and default values for TypeVar, while Python 3.10 introduced the match statement with improved type checking.

#+begin_src python
"""Type hints: static type checking with mypy"""

from typing import List, Dict, Tuple, Optional, Union, TypeVar, Generic
from typing import Protocol, Literal, Final
from dataclasses import dataclass

# Basic type hints
def greet(name: str, age: int) -> str:
    return f"Hello {name}, you are {age} years old"

# Optional types
def find_user(user_id: int) -> Optional[Dict[str, Any]]:
    """Return user or None"""
    if user_id > 0:
        return {'id': user_id, 'name': 'User'}
    return None

# Union types (Python 3.10+ allows: str | int)
def process_id(id: Union[str, int]) -> str:
    return str(id)

# Literal types
def set_mode(mode: Literal['train', 'test', 'eval']) -> None:
    print(f"Mode: {mode}")

# Generic types
T = TypeVar('T')

def first_element(items: List[T]) -> Optional[T]:
    """Get first element of list"""
    return items[0] if items else None

# Dataclasses with type hints
@dataclass
class Point:
    """2D point with type hints"""
    x: float
    y: float
    label: str = "origin"
    
    def distance_from_origin(self) -> float:
        return (self.x**2 + self.y**2) ** 0.5

# Usage
p = Point(3.0, 4.0, "my_point")
print(f"Distance: {p.distance_from_origin():.2f}")
#+end_src

#+begin_src python
"""Advanced typing: Protocols and Generics"""

from typing import Protocol, TypeVar, Generic

# Protocols (structural subtyping)
class Drawable(Protocol):
    """Protocol for drawable objects"""
    def draw(self) -> None: ...

def render(obj: Drawable) -> None:
    """Render any drawable object"""
    obj.draw()

# Generic classes
T = TypeVar('T')

class Stack(Generic[T]):
    """Generic stack implementation"""
    
    def __init__(self) -> None:
        self._items: List[T] = []
    
    def push(self, item: T) -> None:
        self._items.append(item)
    
    def pop(self) -> T:
        return self._items.pop()
    
    def is_empty(self) -> bool:
        return len(self._items) == 0

# Usage
int_stack: Stack[int] = Stack()
int_stack.push(1)
int_stack.push(2)
print(f"Popped: {int_stack.pop()}")
#+end_src

* Object-Oriented Programming
:PROPERTIES:
:CUSTOM_ID: sec:oop
:END:

** Classes and Objects

#+begin_src python
"""Object-Oriented Programming in Python"""

class BankAccount:
    """Bank account with encapsulation"""
    
    # Class variable
    interest_rate = 0.02
    
    def __init__(self, owner: str, balance: float = 0.0):
        self.owner = owner
        self._balance = balance  # Protected attribute
        self.__transactions = []  # Private attribute
    
    @property
    def balance(self) -> float:
        """Get balance (read-only property)"""
        return self._balance
    
    def deposit(self, amount: float) -> None:
        """Deposit money"""
        if amount > 0:
            self._balance += amount
            self.__transactions.append(('deposit', amount))
    
    def withdraw(self, amount: float) -> bool:
        """Withdraw money"""
        if 0 < amount <= self._balance:
            self._balance -= amount
            self.__transactions.append(('withdraw', amount))
            return True
        return False
    
    def apply_interest(self) -> None:
        """Apply interest to balance"""
        self._balance *= (1 + self.interest_rate)
    
    def __str__(self) -> str:
        return f"Account({self.owner}: ${self._balance:.2f})"
    
    def __repr__(self) -> str:
        return f"BankAccount(owner='{self.owner}', balance={self._balance})"

# Usage
account = BankAccount("Alice", 1000)
account.deposit(500)
account.withdraw(200)
account.apply_interest()
print(account)
print(f"Final balance: ${account.balance:.2f}")
#+end_src

** Inheritance and Polymorphism

#+begin_src python
"""Inheritance: code reuse and polymorphism"""

from abc import ABC, abstractmethod

# Abstract base class
class Shape(ABC):
    """Abstract shape class"""
    
    def __init__(self, color: str):
        self.color = color
    
    @abstractmethod
    def area(self) -> float:
        """Calculate area - must be implemented by subclasses"""
        pass
    
    @abstractmethod
    def perimeter(self) -> float:
        """Calculate perimeter - must be implemented by subclasses"""
        pass

class Rectangle(Shape):
    """Rectangle implementation"""
    
    def __init__(self, width: float, height: float, color: str = "white"):
        super().__init__(color)
        self.width = width
        self.height = height
    
    def area(self) -> float:
        return self.width * self.height
    
    def perimeter(self) -> float:
        return 2 * (self.width + self.height)

class Circle(Shape):
    """Circle implementation"""
    
    def __init__(self, radius: float, color: str = "white"):
        super().__init__(color)
        self.radius = radius
    
    def area(self) -> float:
        return np.pi * self.radius ** 2
    
    def perimeter(self) -> float:
        return 2 * np.pi * self.radius

# Polymorphism in action
shapes: List[Shape] = [
    Rectangle(10, 5, "red"),
    Circle(7, "blue"),
    Rectangle(3, 3, "green")
]

for shape in shapes:
    print(f"{shape.__class__.__name__} ({shape.color}): "
          f"Area={shape.area():.2f}, Perimeter={shape.perimeter():.2f}")
#+end_src

** Magic Methods (Dunder Methods)

#+begin_src python
"""Magic methods: customize object behavior"""

class Vector:
    """2D vector with operator overloading"""
    
    def __init__(self, x: float, y: float):
        self.x = x
        self.y = y
    
    def __add__(self, other: 'Vector') -> 'Vector':
        """Vector addition: v1 + v2"""
        return Vector(self.x + other.x, self.y + other.y)
    
    def __mul__(self, scalar: float) -> 'Vector':
        """Scalar multiplication: v * k"""
        return Vector(self.x * scalar, self.y * scalar)
    
    def __abs__(self) -> float:
        """Vector magnitude: abs(v)"""
        return (self.x**2 + self.y**2) ** 0.5
    
    def __eq__(self, other: 'Vector') -> bool:
        """Equality: v1 == v2"""
        return self.x == other.x and self.y == other.y
    
    def __repr__(self) -> str:
        return f"Vector({self.x}, {self.y})"
    
    def __getitem__(self, index: int) -> float:
        """Index access: v[0], v[1]"""
        return (self.x, self.y)[index]

# Usage
v1 = Vector(3, 4)
v2 = Vector(1, 2)
v3 = v1 + v2
v4 = v1 * 2

print(f"v1 = {v1}")
print(f"v2 = {v2}")
print(f"v1 + v2 = {v3}")
print(f"v1 * 2 = {v4}")
print(f"|v1| = {abs(v1):.2f}")
print(f"v1[0] = {v1[0]}, v1[1] = {v1[1]}")
#+end_src

* NumPy: Numerical Computing
:PROPERTIES:
:CUSTOM_ID: sec:numpy
:END:

NumPy is the backbone of numerical computing in Python, providing efficient handling of large multidimensional arrays and mathematical functions, with NumPy 2.0 bringing improved memory usage and better parallelism support.

** Array Creation and Basic Operations

#+begin_src python
"""NumPy fundamentals: arrays and operations"""

import numpy as np

# Creating arrays
arr1d = np.array([1, 2, 3, 4, 5])
arr2d = np.array([[1, 2, 3], [4, 5, 6]])

# Array creation functions
zeros = np.zeros((3, 3))
ones = np.ones((2, 4))
identity = np.eye(3)
arange = np.arange(0, 10, 2)  # Like range()
linspace = np.linspace(0, 1, 5)  # 5 evenly spaced points

print(f"1D Array: {arr1d}")
print(f"2D Array shape: {arr2d.shape}")
print(f"Linspace: {linspace}")

# Random arrays
np.random.seed(42)
random_uniform = np.random.random((3, 3))
random_normal = np.random.randn(1000)  # Standard normal
random_int = np.random.randint(0, 100, size=(3, 3))

print(f"\nRandom uniform array:\n{random_uniform}")
print(f"Random normal mean: {random_normal.mean():.3f}, std: {random_normal.std():.3f}")
#+end_src

** Array Indexing and Slicing

#+begin_src python
"""NumPy indexing: powerful array access"""

# Create sample array
arr = np.arange(20).reshape(4, 5)
print(f"Array:\n{arr}")

# Basic indexing
print(f"\nElement [2, 3]: {arr[2, 3]}")
print(f"First row: {arr[0, :]}")
print(f"Last column: {arr[:, -1]}")

# Boolean indexing
mask = arr > 10
filtered = arr[mask]
print(f"\nElements > 10: {filtered}")

# Fancy indexing
indices = [0, 2, 3]
selected_rows = arr[indices, :]
print(f"\nSelected rows (0, 2, 3):\n{selected_rows}")

# Where function
result = np.where(arr % 2 == 0, arr, -1)  # Keep evens, replace odds with -1
print(f"\nWhere example:\n{result}")
#+end_src

** Broadcasting and Vectorization

#+begin_src python
"""NumPy broadcasting: operations on differently shaped arrays"""

# Broadcasting example 1: vector + scalar
vec = np.array([1, 2, 3, 4])
result = vec + 10  # Scalar broadcast to vector
print(f"Vector + scalar: {result}")

# Broadcasting example 2: matrix + vector
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])
row_vec = np.array([10, 20, 30])
result = matrix + row_vec  # row_vec broadcast to each row
print(f"Matrix + row vector:\n{result}")

# Outer product using broadcasting
a = np.array([1, 2, 3, 4])
b = np.array([10, 20, 30])
outer = a[:, np.newaxis] * b  # Shape (4,1) * (3,) -> (4,3)
print(f"Outer product:\n{outer}")

# Vectorization: avoid loops!
def slow_sum_squares(arr):
    """Slow Python loop"""
    result = 0
    for x in arr:
        result += x**2
    return result

def fast_sum_squares(arr):
    """Fast NumPy vectorization"""
    return np.sum(arr**2)

large_arr = np.random.random(10000)
print(f"\nVectorization much faster than loops!")
#+end_src

** Linear Algebra

#+begin_src python
"""NumPy linear algebra: essential operations"""

# Matrix operations
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix multiplication
C = A @ B  # Or np.matmul(A, B) or np.dot(A, B)
print(f"Matrix product A @ B:\n{C}")

# Element-wise operations
elementwise = A * B  # Not matrix multiplication!
print(f"Element-wise product:\n{elementwise}")

# Matrix properties
det_A = np.linalg.det(A)
inv_A = np.linalg.inv(A)
eigenvalues, eigenvectors = np.linalg.eig(A)

print(f"\nDeterminant of A: {det_A:.2f}")
print(f"Eigenvalues: {eigenvalues}")

# Solving linear systems: Ax = b
b = np.array([5, 11])
x = np.linalg.solve(A, b)
print(f"Solution to Ax=b: {x}")
print(f"Verification: Ax = {A @ x}")
#+end_src

** Advanced Array Operations

#+begin_src python
"""Advanced NumPy: reshape, stack, split"""

# Reshaping
arr = np.arange(12)
reshaped = arr.reshape(3, 4)
print(f"Reshaped to 3x4:\n{reshaped}")

# Flattening
flattened = reshaped.ravel()  # or .flatten()
print(f"Flattened: {flattened}")

# Stacking arrays
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
vstacked = np.vstack([a, b])  # Vertical stack
hstacked = np.hstack([a, b])  # Horizontal stack

print(f"Vertical stack:\n{vstacked}")
print(f"Horizontal stack: {hstacked}")

# Splitting arrays
arr = np.arange(16).reshape(4, 4)
top, bottom = np.vsplit(arr, 2)
left, right = np.hsplit(arr, 2)

print(f"\nOriginal:\n{arr}")
print(f"Top half:\n{top}")

# Transpose and swapping axes
transposed = arr.T
print(f"Transposed:\n{transposed}")
#+end_src

* SciPy: Scientific Computing
:PROPERTIES:
:CUSTOM_ID: sec:scipy
:END:

SciPy extends NumPy with additional mathematical functions, optimization algorithms, and statistical tools for scientific and technical computing.

** Optimization

#+begin_src python
"""SciPy optimization: finding minima and maxima"""

from scipy import optimize

# Minimize a function
def objective(x):
    """Function to minimize: (x-3)^2 + (y-2)^2"""
    return (x[0] - 3)**2 + (x[1] - 2)**2

# Starting point
x0 = np.array([0, 0])

# Minimize
result = optimize.minimize(objective, x0, method='BFGS')
print(f"Minimum found at: {result.x}")
print(f"Minimum value: {result.fun:.6f}")

# Curve fitting
def model(x, a, b, c):
    """Model: y = a*exp(-b*x) + c"""
    return a * np.exp(-b * x) + c

# Generate noisy data
x_data = np.linspace(0, 4, 50)
y_data = model(x_data, 2.5, 1.3, 0.5) + 0.2 * np.random.randn(50)

# Fit parameters
params, covariance = optimize.curve_fit(model, x_data, y_data)
print(f"\nFitted parameters: a={params[0]:.2f}, b={params[1]:.2f}, c={params[2]:.2f}")
#+end_src

** Integration and Differential Equations

#+begin_src python
"""SciPy integration and ODEs"""

from scipy import integrate

# Numerical integration
def integrand(x):
    return np.sin(x)

result, error = integrate.quad(integrand, 0, np.pi)
print(f"∫sin(x)dx from 0 to π = {result:.6f} (error: {error:.2e})")

# Solving ODEs: dy/dt = -y + sin(t)
def dydt(y, t):
    return -y + np.sin(t)

y0 = 0
t = np.linspace(0, 10, 100)
y = integrate.odeint(dydt, y0, t)

print(f"ODE solution at t=10: y = {y[-1][0]:.4f}")
#+end_src

** Interpolation

#+begin_src python
"""SciPy interpolation: estimate between data points"""

from scipy import interpolate

# Original data
x = np.array([0, 1, 2, 3, 4, 5])
y = np.array([0, 1, 4, 2, 5, 3])

# Create interpolation functions
f_linear = interpolate.interp1d(x, y, kind='linear')
f_cubic = interpolate.interp1d(x, y, kind='cubic')

# Interpolate at new points
x_new = np.linspace(0, 5, 50)
y_linear = f_linear(x_new)
y_cubic = f_cubic(x_new)

print(f"Interpolated value at x=2.5 (linear): {f_linear(2.5):.2f}")
print(f"Interpolated value at x=2.5 (cubic): {f_cubic(2.5):.2f}")
#+end_src

** Statistics

#+begin_src python
"""SciPy statistics: distributions and tests"""

from scipy import stats

# Normal distribution
mu, sigma = 0, 1
x = np.linspace(-4, 4, 100)
pdf = stats.norm.pdf(x, mu, sigma)
cdf = stats.norm.cdf(x, mu, sigma)

print(f"P(X < 1.96) for N(0,1): {stats.norm.cdf(1.96):.4f}")

# Generate random samples
samples = stats.norm.rvs(loc=mu, scale=sigma, size=1000)
print(f"Sample mean: {samples.mean():.3f}, std: {samples.std():.3f}")

# Statistical tests
# t-test: are two samples different?
sample1 = stats.norm.rvs(loc=0, scale=1, size=100)
sample2 = stats.norm.rvs(loc=0.5, scale=1, size=100)
t_stat, p_value = stats.ttest_ind(sample1, sample2)

print(f"\nt-test: t={t_stat:.3f}, p={p_value:.4f}")
if p_value < 0.05:
    print("Samples are significantly different")
else:
    print("No significant difference")
#+end_src

* Pandas: Data Analysis
:PROPERTIES:
:CUSTOM_ID: sec:pandas
:END:

Pandas is the gold standard for data manipulation and analysis, offering DataFrame and Series objects for handling tabular data with comprehensive tools for cleaning, transformation, and time series analysis.

** DataFrames: Creation and Basic Operations

#+begin_src python
"""Pandas DataFrames: structured data manipulation"""

import pandas as pd

# Create DataFrame from dictionary
data = {
    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
    'age': [25, 30, 35, 28, 32],
    'city': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'],
    'salary': [70000, 80000, 90000, 75000, 85000]
}
df = pd.DataFrame(data)

print("DataFrame:")
print(df)
print(f"\nShape: {df.shape}")
print(f"\nColumn types:\n{df.dtypes}")
print(f"\nBasic statistics:\n{df.describe()}")

# Selecting data
print(f"\nNames: {df['name'].tolist()}")
print(f"\nFirst 3 rows:\n{df.head(3)}")
print(f"\nRow 2:\n{df.iloc[2]}")

# Filtering
high_earners = df[df['salary'] > 80000]
print(f"\nHigh earners:\n{high_earners}")
#+end_src

** Data Cleaning and Transformation

#+begin_src python
"""Pandas: cleaning and transforming data"""

# Create DataFrame with missing values
data = {
    'A': [1, 2, np.nan, 4, 5],
    'B': [10, np.nan, 30, 40, 50],
    'C': ['a', 'b', 'c', 'd', 'e']
}
df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Handling missing values
print(f"\nMissing values per column:\n{df.isnull().sum()}")

# Fill missing values
df_filled = df.fillna(method='ffill')  # Forward fill
df_filled_value = df.fillna(0)  # Fill with 0

# Drop rows with missing values
df_dropped = df.dropna()

print(f"\nForward filled:\n{df_filled}")

# Adding columns
df['D'] = df['A'] * 2
df['E'] = df['A'] + df['B']

# Apply functions
df['A_squared'] = df['A'].apply(lambda x: x**2 if pd.notna(x) else np.nan)

print(f"\nWith new columns:\n{df.head()}")
#+end_src

** Grouping and Aggregation

#+begin_src python
"""Pandas groupby: split-apply-combine"""

# Create sample sales data
np.random.seed(42)
sales_data = pd.DataFrame({
    'product': np.random.choice(['A', 'B', 'C'], 100),
    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),
    'sales': np.random.randint(100, 1000, 100),
    'quantity': np.random.randint(1, 20, 100)
})

print("Sales Data:")
print(sales_data.head())

# Group by single column
by_product = sales_data.groupby('product')['sales'].agg(['sum', 'mean', 'count'])
print(f"\nBy Product:\n{by_product}")

# Group by multiple columns
by_product_region = sales_data.groupby(['product', 'region'])['sales'].sum()
print(f"\nBy Product and Region:\n{by_product_region}")

# Pivot tables
pivot = pd.pivot_table(sales_data, 
                       values='sales', 
                       index='product', 
                       columns='region',
                       aggfunc='sum',
                       fill_value=0)
print(f"\nPivot Table:\n{pivot}")
#+end_src

** Merging and Joining

#+begin_src python
"""Pandas: combining DataFrames"""

# Create sample DataFrames
customers = pd.DataFrame({
    'customer_id': [1, 2, 3, 4],
    'name': ['Alice', 'Bob', 'Charlie', 'David'],
    'city': ['NYC', 'LA', 'Chicago', 'Houston']
})

orders = pd.DataFrame({
    'order_id': [101, 102, 103, 104, 105],
    'customer_id': [1, 2, 1, 3, 2],
    'amount': [100, 200, 150, 300, 250]
})

# Inner join
merged = pd.merge(customers, orders, on='customer_id', how='inner')
print("Inner Join:")
print(merged)

# Left join
left_merged = pd.merge(customers, orders, on='customer_id', how='left')
print(f"\nLeft Join:\n{left_merged}")

# Concatenation
df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})
concatenated = pd.concat([df1, df2], ignore_index=True)
print(f"\nConcatenated:\n{concatenated}")
#+end_src

** Time Series

#+begin_src python
"""Pandas: time series analysis"""

# Create time series data
dates = pd.date_range('2024-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'date': dates,
    'value': np.random.randn(100).cumsum() + 100
})
ts_data.set_index('date', inplace=True)

print("Time Series Data:")
print(ts_data.head())

# Resampling
monthly = ts_data.resample('M').mean()
print(f"\nMonthly average:\n{monthly.head()}")

# Rolling window
ts_data['rolling_mean'] = ts_data['value'].rolling(window=7).mean()
ts_data['rolling_std'] = ts_data['value'].rolling(window=7).std()

print(f"\nWith rolling statistics:\n{ts_data.head(10)}")

# Time-based indexing
jan_data = ts_data.loc['2024-01']
print(f"\nJanuary data shape: {jan_data.shape}")
#+end_src

* Data Visualization
:PROPERTIES:
:CUSTOM_ID: sec:visualization
:END:

Matplotlib provides the foundation for static, animated, and interactive visualizations, while Seaborn builds on it to offer beautiful statistical plots with minimal code and built-in themes.

** Matplotlib Basics

#+begin_src python :results file :file ./figures/matplotlib_basics.png
"""Matplotlib: fundamental plotting"""

import matplotlib.pyplot as plt

# Create figure and axes
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('Matplotlib Basic Plots', fontsize=16, fontweight='bold')

# Line plot
x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)
axes[0, 0].plot(x, y1, 'b-', label='sin(x)', linewidth=2)
axes[0, 0].plot(x, y2, 'r--', label='cos(x)', linewidth=2)
axes[0, 0].set_title('Line Plot')
axes[0, 0].set_xlabel('x')
axes[0, 0].set_ylabel('y')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Scatter plot
np.random.seed(42)
x_scatter = np.random.randn(100)
y_scatter = 2 * x_scatter + np.random.randn(100)
colors = np.random.rand(100)
sizes = 100 * np.random.rand(100)
axes[0, 1].scatter(x_scatter, y_scatter, c=colors, s=sizes, alpha=0.6, cmap='viridis')
axes[0, 1].set_title('Scatter Plot')
axes[0, 1].set_xlabel('x')
axes[0, 1].set_ylabel('y')

# Bar plot
categories = ['A', 'B', 'C', 'D', 'E']
values = [23, 45, 56, 78, 32]
axes[1, 0].bar(categories, values, color='steelblue', alpha=0.7)
axes[1, 0].set_title('Bar Plot')
axes[1, 0].set_ylabel('Values')

# Histogram
data = np.random.randn(1000)
axes[1, 1].hist(data, bins=30, color='green', alpha=0.7, edgecolor='black')
axes[1, 1].set_title('Histogram')
axes[1, 1].set_xlabel('Value')
axes[1, 1].set_ylabel('Frequency')

plt.tight_layout()
plt.savefig('./figures/matplotlib_basics.png', dpi=300, bbox_inches='tight')
return './figures/matplotlib_basics.png'
#+end_src

** Seaborn: Statistical Visualization

#+begin_src python :results file :file ./figures/seaborn_plots.png
"""Seaborn: beautiful statistical visualizations"""

import seaborn as sns

# Set style
sns.set_style("whitegrid")
sns.set_palette("husl")

# Load sample dataset
iris = sns.load_dataset('iris')

# Create figure
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
fig.suptitle('Seaborn Statistical Plots', fontsize=16, fontweight='bold')

# Distribution plot
sns.histplot(data=iris, x='sepal_length', hue='species', 
             kde=True, ax=axes[0, 0], alpha=0.6)
axes[0, 0].set_title('Distribution Plot with KDE')

# Box plot
sns.boxplot(data=iris, x='species', y='petal_length', ax=axes[0, 1])
axes[0, 1].set_title('Box Plot')

# Violin plot
sns.violinplot(data=iris, x='species', y='petal_width', ax=axes[1, 0])
axes[1, 0].set_title('Violin Plot')

# Pair plot (in separate axis)
# For pair plot, we'll show correlation heatmap instead
corr_matrix = iris.drop('species', axis=1).corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            ax=axes[1, 1], square=True)
axes[1, 1].set_title('Correlation Heatmap')

plt.tight_layout()
plt.savefig('./figures/seaborn_plots.png', dpi=300, bbox_inches='tight')
return './figures/seaborn_plots.png'
#+end_src

** Advanced Matplotlib

#+begin_src python :results file :file ./figures/advanced_matplotlib.png
"""Advanced Matplotlib: 3D plots and subplots"""

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(14, 10))
fig.suptitle('Advanced Matplotlib Visualizations', fontsize=16, fontweight='bold')

# 3D surface plot
ax1 = fig.add_subplot(221, projection='3d')
x = np.linspace(-5, 5, 50)
y = np.linspace(-5, 5, 50)
X, Y = np.meshgrid(x, y)
Z = np.sin(np.sqrt(X**2 + Y**2))
surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
ax1.set_title('3D Surface Plot')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.set_zlabel('Z')
fig.colorbar(surf, ax=ax1, shrink=0.5)

# Contour plot
ax2 = fig.add_subplot(222)
contour = ax2.contourf(X, Y, Z, levels=20, cmap='RdYlBu')
ax2.contour(X, Y, Z, levels=20, colors='black', linewidths=0.5, alpha=0.3)
ax2.set_title('Contour Plot')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
fig.colorbar(contour, ax=ax2)

# Polar plot
ax3 = fig.add_subplot(223, projection='polar')
theta = np.linspace(0, 2*np.pi, 100)
r = np.abs(np.sin(3*theta))
ax3.plot(theta, r, 'b-', linewidth=2)
ax3.fill(theta, r, alpha=0.3)
ax3.set_title('Polar Plot')

# Stream plot
ax4 = fig.add_subplot(224)
x = np.linspace(-3, 3, 20)
y = np.linspace(-3, 3, 20)
X, Y = np.meshgrid(x, y)
U = -Y
V = X
speed = np.sqrt(U**2 + V**2)
ax4.streamplot(X, Y, U, V, color=speed, cmap='autumn', density=1.5)
ax4.set_title('Stream Plot (Vector Field)')
ax4.set_xlabel('X')
ax4.set_ylabel('Y')

plt.tight_layout()
plt.savefig('./figures/advanced_matplotlib.png', dpi=300, bbox_inches='tight')
return './figures/advanced_matplotlib.png'
#+end_src

* Scikit-learn: Machine Learning
:PROPERTIES:
:CUSTOM_ID: sec:sklearn
:END:

Scikit-learn is the go-to library for classical machine learning, providing simple and efficient tools for data mining, classification, regression, clustering, and dimensionality reduction.

** Supervised Learning: Classification

#+begin_src python
"""Scikit-learn: classification with multiple algorithms"""

from sklearn.datasets import load_iris, make_classification
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train multiple classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=200),
    'Decision Tree': DecisionTreeClassifier(max_depth=5),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf'),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

print("Classification Results:")
print("=" * 60)

for name, clf in classifiers.items():
    # Train
    clf.fit(X_train_scaled, y_train)
    
    # Predict
    y_pred = clf.predict(X_test_scaled)
    
    # Evaluate
    accuracy = accuracy_score(y_test, y_pred)
    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)
    
    print(f"{name:25} Accuracy: {accuracy:.4f} | CV: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

print("=" * 60)
#+end_src

#+begin_src python
"""Classification metrics and confusion matrix"""

from sklearn.metrics import precision_score, recall_score, f1_score

# Train Random Forest for detailed analysis
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)
y_pred = rf.predict(X_test_scaled)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# Feature importance
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\nFeature Importance:")
print(feature_importance)
#+end_src

** Supervised Learning: Regression

#+begin_src python
"""Scikit-learn: regression analysis"""

from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Generate regression dataset
X, y = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)

# Split and scale
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train multiple regressors
regressors = {
    'Linear Regression': LinearRegression(),
    'Ridge (L2)': Ridge(alpha=1.0),
    'Lasso (L1)': Lasso(alpha=0.1),
    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

print("Regression Results:")
print("=" * 80)
print(f"{'Model':<25} {'RMSE':<12} {'MAE':<12} {'R² Score':<12}")
print("-" * 80)

for name, reg in regressors.items():
    # Train
    reg.fit(X_train_scaled, y_train)
    
    # Predict
    y_pred = reg.predict(X_test_scaled)
    
    # Metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"{name:<25} {rmse:<12.4f} {mae:<12.4f} {r2:<12.4f}")

print("=" * 80)
#+end_src

** Unsupervised Learning: Clustering

#+begin_src python
"""Scikit-learn: clustering algorithms"""

from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Generate clustering dataset
X, y_true = make_blobs(n_samples=500, centers=4, n_features=2, 
                       cluster_std=1.0, random_state=42)

# K-Means clustering
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
y_dbscan = dbscan.fit_predict(X)

# Hierarchical clustering
hierarchical = AgglomerativeClustering(n_clusters=4)
y_hierarchical = hierarchical.fit_predict(X)

# Evaluate clustering
algorithms = [
    ('K-Means', y_kmeans),
    ('DBSCAN', y_dbscan),
    ('Hierarchical', y_hierarchical)
]

print("Clustering Results:")
print("=" * 60)
print(f"{'Algorithm':<20} {'Silhouette':<15} {'Davies-Bouldin':<15}")
print("-" * 60)

for name, labels in algorithms:
    if len(np.unique(labels)) > 1:  # Need at least 2 clusters
        silhouette = silhouette_score(X, labels)
        davies_bouldin = davies_bouldin_score(X, labels)
        print(f"{name:<20} {silhouette:<15.4f} {davies_bouldin:<15.4f}")

print("=" * 60)
print(f"\nK-Means cluster centers:\n{kmeans.cluster_centers_}")
#+end_src

** Dimensionality Reduction

#+begin_src python
"""Scikit-learn: dimensionality reduction techniques"""

from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Load high-dimensional data
iris = load_iris()
X, y = iris.data, iris.target

# PCA (Principal Component Analysis)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print("PCA Results:")
print(f"Explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}")

# t-SNE (t-Distributed Stochastic Neighbor Embedding)
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# LDA (Linear Discriminant Analysis) - supervised
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)

print(f"\nOriginal dimensions: {X.shape}")
print(f"PCA dimensions: {X_pca.shape}")
print(f"t-SNE dimensions: {X_tsne.shape}")
print(f"LDA dimensions: {X_lda.shape}")

# Feature extraction with PCA
pca_full = PCA()
pca_full.fit(X)
cumsum = np.cumsum(pca_full.explained_variance_ratio_)
n_components_95 = np.argmax(cumsum >= 0.95) + 1
print(f"\nComponents needed for 95% variance: {n_components_95}")
#+end_src

** Model Selection and Hyperparameter Tuning

#+begin_src python
"""Scikit-learn: cross-validation and hyperparameter tuning"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import learning_curve
from scipy.stats import uniform, randint

# Load data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Grid Search for SVM
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'kernel': ['rbf', 'poly']
}

svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

print("Grid Search Results:")
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")
print(f"Test score: {grid_search.score(X_test, y_test):.4f}")

# Random Search for Random Forest
param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(3, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

rf = RandomForestClassifier(random_state=42)
random_search = RandomizedSearchCV(rf, param_dist, n_iter=20, cv=5, 
                                   scoring='accuracy', random_state=42, n_jobs=-1)
random_search.fit(X_train, y_train)

print(f"\nRandom Search Results:")
print(f"Best parameters: {random_search.best_params_}")
print(f"Best cross-validation score: {random_search.best_score_:.4f}")
#+end_src

** Pipeline and Feature Engineering

#+begin_src python
"""Scikit-learn: pipelines for workflow automation"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif

# Create a pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('feature_selection', SelectKBest(f_classif, k=3)),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Train pipeline
pipeline.fit(X_train, y_train)

# Evaluate
score = pipeline.score(X_test, y_test)
print(f"Pipeline accuracy: {score:.4f}")

# Feature engineering pipeline
poly_pipeline = Pipeline([
    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000))
])

poly_pipeline.fit(X_train, y_train)
poly_score = poly_pipeline.score(X_test, y_test)
print(f"Polynomial features pipeline accuracy: {poly_score:.4f}")

# Get selected features
selected_features = pipeline.named_steps['feature_selection'].get_support()
feature_names = np.array(iris.feature_names)
print(f"\nSelected features: {feature_names[selected_features]}")
#+end_src

* Deep Learning with PyTorch
:PROPERTIES:
:CUSTOM_ID: sec:pytorch
:END:

PyTorch has become the dominant framework for deep learning research and production, offering dynamic computational graphs, intuitive APIs, and seamless GPU acceleration.

** PyTorch Basics: Tensors

#+begin_src python
"""PyTorch fundamentals: tensors and operations"""

import torch
import torch.nn as nn
import torch.optim as optim

# Create tensors
x = torch.tensor([1, 2, 3, 4, 5])
y = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)

print(f"1D Tensor: {x}")
print(f"2D Tensor:\n{y}")
print(f"Shape: {y.shape}, dtype: {y.dtype}")

# Tensor operations
a = torch.randn(3, 4)
b = torch.randn(3, 4)

# Element-wise operations
c = a + b
d = a * b
e = torch.matmul(a, b.T)  # Matrix multiplication

print(f"\nMatrix multiplication shape: {e.shape}")

# GPU support (if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nUsing device: {device}")

# Move tensor to device
x_gpu = x.to(device)

# Common tensor operations
zeros = torch.zeros(3, 3)
ones = torch.ones(2, 4)
random = torch.randn(3, 3)  # Normal distribution
uniform = torch.rand(3, 3)  # Uniform [0, 1)

# Reshaping
reshaped = random.view(1, 9)  # Like reshape
print(f"Reshaped: {reshaped.shape}")
#+end_src

** Building Neural Networks

#+begin_src python
"""PyTorch: building neural networks"""

# Define a simple feedforward network
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        return out

# Instantiate model
input_size = 784  # 28x28 images
hidden_size = 128
num_classes = 10
model = SimpleNN(input_size, hidden_size, num_classes)

print("Model architecture:")
print(model)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
#+end_src

#+begin_src python
"""PyTorch: training a neural network"""

# Generate dummy data
X_train = torch.randn(1000, input_size)
y_train = torch.randint(0, num_classes, (1000,))
X_val = torch.randn(200, input_size)
y_val = torch.randint(0, num_classes, (200,))

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10
batch_size = 32

print("\nTraining neural network...")
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    # Mini-batch training
    for i in range(0, len(X_train), batch_size):
        # Get batch
        batch_X = X_train[i:i+batch_size]
        batch_y = y_train[i:i+batch_size]
        
        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Statistics
        total_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += batch_y.size(0)
        correct += (predicted == batch_y).sum().item()
    
    # Validation
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val)
        val_loss = criterion(val_outputs, y_val)
        _, val_predicted = torch.max(val_outputs.data, 1)
        val_accuracy = (val_predicted == y_val).sum().item() / len(y_val)
    
    if (epoch + 1) % 2 == 0:
        train_accuracy = 100 * correct / total
        print(f'Epoch [{epoch+1}/{num_epochs}], '
              f'Loss: {total_loss/(i//batch_size+1):.4f}, '
              f'Train Acc: {train_accuracy:.2f}%, '
              f'Val Acc: {100*val_accuracy:.2f}%')
#+end_src

** Convolutional Neural Networks (CNN)

#+begin_src python
"""PyTorch: CNN for image classification"""

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()
        
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # Pooling
        self.pool = nn.MaxPool2d(2, 2)
        
        # Fully connected layers
        self.fc1 = nn.Linear(128 * 3 * 3, 256)
        self.fc2 = nn.Linear(256, num_classes)
        
        # Dropout and batch norm
        self.dropout = nn.Dropout(0.5)
        self.batch_norm1 = nn.BatchNorm2d(32)
        self.batch_norm2 = nn.BatchNorm2d(64)
        self.batch_norm3 = nn.BatchNorm2d(128)
    
    def forward(self, x):
        # Conv block 1
        x = self.conv1(x)
        x = self.batch_norm1(x)
        x = nn.functional.relu(x)
        x = self.pool(x)
        
        # Conv block 2
        x = self.conv2(x)
        x = self.batch_norm2(x)
        x = nn.functional.relu(x)
        x = self.pool(x)
        
        # Conv block 3
        x = self.conv3(x)
        x = self.batch_norm3(x)
        x = nn.functional.relu(x)
        x = self.pool(x)
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # Fully connected
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# Create CNN
cnn = CNN(num_classes=10)
print("CNN Architecture:")
print(cnn)

# Test forward pass
dummy_input = torch.randn(16, 1, 28, 28)  # Batch of 16 images
output = cnn(dummy_input)
print(f"\nInput shape: {dummy_input.shape}")
print(f"Output shape: {output.shape}")
#+end_src

** Recurrent Neural Networks (RNN/LSTM)

#+begin_src python
"""PyTorch: RNN and LSTM for sequence data"""

class LSTM_Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTM_Model, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        
        # Fully connected layer
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        # Initialize hidden state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        
        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # Get output from last time step
        out = self.fc(out[:, -1, :])
        
        return out

# Create LSTM model
input_size = 28  # Treat image rows as sequence
hidden_size = 128
num_layers = 2
num_classes = 10

lstm_model = LSTM_Model(input_size, hidden_size, num_layers, num_classes)
print("LSTM Model:")
print(lstm_model)

# Test sequence input
sequence = torch.randn(32, 28, 28)  # Batch=32, Seq=28, Features=28
output = lstm_model(sequence)
print(f"\nSequence shape: {sequence.shape}")
print(f"Output shape: {output.shape}")
#+end_src

* TensorFlow and Keras
:PROPERTIES:
:CUSTOM_ID: sec:tensorflow
:END:

TensorFlow, with its high-level Keras API, remains popular for production deployment, offering robust tools for both research and serving machine learning models at scale.

** Keras Sequential API

#+begin_src python
"""TensorFlow/Keras: building models with Sequential API"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Set random seed
tf.random.set_seed(42)

# Build sequential model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# Compile model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Model summary
print("Model Summary:")
model.summary()

# Generate dummy data
X_train = np.random.randn(1000, 784).astype(np.float32)
y_train = np.random.randint(0, 10, 1000)
X_val = np.random.randn(200, 784).astype(np.float32)
y_val = np.random.randint(0, 10, 200)

# Train model
print("\nTraining model...")
history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=5,
    validation_data=(X_val, y_val),
    verbose=1
)

# Evaluate
test_loss, test_acc = model.evaluate(X_val, y_val, verbose=0)
print(f"\nTest accuracy: {test_acc:.4f}")
#+end_src

** Keras Functional API

#+begin_src python
"""TensorFlow/Keras: Functional API for complex architectures"""

# Input layer
inputs = keras.Input(shape=(784,))

# First branch
x1 = layers.Dense(128, activation='relu')(inputs)
x1 = layers.Dropout(0.3)(x1)
x1 = layers.Dense(64, activation='relu')(x1)

# Second branch
x2 = layers.Dense(64, activation='relu')(inputs)
x2 = layers.Dropout(0.2)(x2)

# Concatenate branches
concatenated = layers.concatenate([x1, x2])

# Output layer
outputs = layers.Dense(10, activation='softmax')(concatenated)

# Create model
functional_model = keras.Model(inputs=inputs, outputs=outputs, name='functional_model')

# Compile
functional_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("Functional Model:")
functional_model.summary()
#+end_src

** Custom Training Loops

#+begin_src python
"""TensorFlow: custom training loop for fine control"""

# Create simple model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(784,)),
    layers.Dense(10, activation='softmax')
])

# Loss function and optimizer
loss_fn = keras.losses.SparseCategoricalCrossentropy()
optimizer = keras.optimizers.Adam()

# Metrics
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

# Custom training step
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)
    
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    
    return loss_value

# Custom validation step
@tf.function
def val_step(x, y):
    val_logits = model(x, training=False)
    val_acc_metric.update_state(y, val_logits)

# Training loop
epochs = 3
batch_size = 32

print("Custom training loop...")
for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    
    # Training
    for i in range(0, len(X_train), batch_size):
        batch_x = X_train[i:i+batch_size]
        batch_y = y_train[i:i+batch_size]
        loss = train_step(batch_x, batch_y)
    
    train_acc = train_acc_metric.result()
    print(f"Training accuracy: {float(train_acc):.4f}")
    train_acc_metric.reset_states()
    
    # Validation
    for i in range(0, len(X_val), batch_size):
        batch_x = X_val[i:i+batch_size]
        batch_y = y_val[i:i+batch_size]
        val_step(batch_x, batch_y)
    
    val_acc = val_acc_metric.result()
    print(f"Validation accuracy: {float(val_acc):.4f}")
    val_acc_metric.reset_states()
#+end_src

* Advanced Topics
:PROPERTIES:
:CUSTOM_ID: sec:advanced_topics
:END:

** Performance Optimization

#+begin_src python
"""Python performance optimization techniques"""

import timeit
from numba import jit
import cProfile
import pstats

# 1. List comprehensions vs loops
def with_loop(n):
    result = []
    for i in range(n):
        result.append(i**2)
    return result

def with_comprehension(n):
    return [i**2 for i in range(n)]

n = 10000
time_loop = timeit.timeit(lambda: with_loop(n), number=100)
time_comp = timeit.timeit(lambda: with_comprehension(n), number=100)

print("Performance Comparison:")
print(f"Loop: {time_loop:.4f}s")
print(f"Comprehension: {time_comp:.4f}s")
print(f"Speedup: {time_loop/time_comp:.2f}x")

# 2. NumPy vectorization
def python_sum_squares(arr):
    return sum([x**2 for x in arr])

def numpy_sum_squares(arr):
    return np.sum(arr**2)

arr_list = list(range(100000))
arr_numpy = np.arange(100000)

time_python = timeit.timeit(lambda: python_sum_squares(arr_list), number=10)
time_numpy = timeit.timeit(lambda: numpy_sum_squares(arr_numpy), number=10)

print(f"\nPython sum: {time_python:.4f}s")
print(f"NumPy sum: {time_numpy:.4f}s")
print(f"Speedup: {time_python/time_numpy:.2f}x")

# 3. Numba JIT compilation
@jit(nopython=True)
def numba_function(n):
    total = 0
    for i in range(n):
        total += i**2
    return total

def python_function(n):
    total = 0
    for i in range(n):
        total += i**2
    return total

# Warm up numba
numba_function(100)

time_python = timeit.timeit(lambda: python_function(100000), number=100)
time_numba = timeit.timeit(lambda: numba_function(100000), number=100)

print(f"\nPython function: {time_python:.4f}s")
print(f"Numba JIT: {time_numba:.4f}s")
print(f"Speedup: {time_python/time_numba:.2f}x")
#+end_src

** Multiprocessing and Concurrency

#+begin_src python
"""Parallel processing with multiprocessing"""

from multiprocessing import Pool, cpu_count
import concurrent.futures

def expensive_computation(n):
    """Simulate expensive computation"""
    return sum([i**2 for i in range(n)])

# Sequential execution
numbers = [1000000] * 8

print(f"CPU count: {cpu_count()}")

# Sequential
start = timeit.default_timer()
results_sequential = [expensive_computation(n) for n in numbers]
time_sequential = timeit.default_timer() - start
print(f"Sequential: {time_sequential:.4f}s")

# Parallel with Pool
start = timeit.default_timer()
with Pool(processes=4) as pool:
    results_parallel = pool.map(expensive_computation, numbers)
time_parallel = timeit.default_timer() - start
print(f"Parallel (Pool): {time_parallel:.4f}s")
print(f"Speedup: {time_sequential/time_parallel:.2f}x")

# Using concurrent.futures
start = timeit.default_timer()
with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
    results_futures = list(executor.map(expensive_computation, numbers))
time_futures = timeit.default_timer() - start
print(f"Parallel (futures): {time_futures:.4f}s")
#+end_src

** Error Handling and Logging

#+begin_src python
"""Best practices for error handling and logging"""

import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Custom exceptions
class DataValidationError(Exception):
    """Custom exception for data validation"""
    pass

def process_data(data):
    """Example function with proper error handling"""
    try:
        if not isinstance(data, list):
            raise TypeError("Data must be a list")
        
        if len(data) == 0:
            raise DataValidationError("Data cannot be empty")
        
        logger.info(f"Processing {len(data)} items")
        result = sum(data)
        logger.info(f"Processing complete: sum = {result}")
        return result
        
    except TypeError as e:
        logger.error(f"Type error: {e}")
        raise
    except DataValidationError as e:
        logger.warning(f"Validation error: {e}")
        return None
    except Exception as e:
        logger.exception(f"Unexpected error: {e}")
        raise
    finally:
        logger.debug("Cleanup completed")

# Test error handling
try:
    result1 = process_data([1, 2, 3, 4, 5])
    print(f"Result: {result1}")
    
    result2 = process_data([])
    print(f"Result: {result2}")
except Exception as e:
    print(f"Error caught: {e}")
#+end_src

** Testing with pytest

#+begin_src python
"""Unit testing examples (pytest style)"""

# Note: This demonstrates pytest patterns
# In practice, these would be in separate test files

def add(a, b):
    """Simple addition function"""
    return a + b

def divide(a, b):
    """Division with error handling"""
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b

class Calculator:
    """Simple calculator class"""
    def __init__(self):
        self.history = []
    
    def add(self, a, b):
        result = a + b
        self.history.append(f"{a} + {b} = {result}")
        return result

# Test functions (would normally be in test_*.py files)
# def test_add():
#     assert add(2, 3) == 5
#     assert add(-1, 1) == 0
#     assert add(0, 0) == 0

# def test_divide():
#     assert divide(10, 2) == 5
#     assert divide(7, 2) == 3.5
#     
#     # Test exception
#     import pytest
#     with pytest.raises(ValueError):
#         divide(1, 0)

# def test_calculator():
#     calc = Calculator()
#     assert calc.add(2, 3) == 5
#     assert len(calc.history) == 1

print("Test examples defined. Run with: pytest test_file.py")
print("Use fixtures, parametrize, and mocking for advanced testing")
#+end_src

* Best Practices and Design Patterns
:PROPERTIES:
:CUSTOM_ID: sec:best_practices
:END:

** Code Style and PEP 8

#+begin_src python
"""Python code style guide (PEP 8)"""

# Good: Descriptive names, proper spacing
def calculate_average(numbers: List[float]) -> float:
    """
    Calculate the average of a list of numbers.
    
    Args:
        numbers: List of numerical values
    
    Returns:
        The arithmetic mean
    
    Raises:
        ValueError: If the list is empty
    """
    if not numbers:
        raise ValueError("Cannot calculate average of empty list")
    
    return sum(numbers) / len(numbers)

# Constants in UPPER_CASE
MAX_RETRIES = 3
DEFAULT_TIMEOUT = 30

# Class names in PascalCase
class DataProcessor:
    """Process and transform data."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self._cache = {}  # Private attribute with underscore
    
    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """Process the data."""
        # Method implementation
        return data

# Function and variable names in snake_case
user_name = "Alice"
total_count = 42

print("Follow PEP 8 for consistent, readable code!")
#+end_src

** Design Patterns

#+begin_src python
"""Common design patterns in Python"""

from abc import ABC, abstractmethod

# 1. Singleton Pattern
class Singleton:
    """Ensure only one instance exists"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

# 2. Factory Pattern
class Animal(ABC):
    @abstractmethod
    def speak(self):
        pass

class Dog(Animal):
    def speak(self):
        return "Woof!"

class Cat(Animal):
    def speak(self):
        return "Meow!"

class AnimalFactory:
    @staticmethod
    def create_animal(animal_type: str) -> Animal:
        if animal_type == "dog":
            return Dog()
        elif animal_type == "cat":
            return Cat()
        else:
            raise ValueError(f"Unknown animal type: {animal_type}")

# 3. Observer Pattern
class Observer(ABC):
    @abstractmethod
    def update(self, message: str):
        pass

class Subject:
    def __init__(self):
        self._observers: List[Observer] = []
    
    def attach(self, observer: Observer):
        self._observers.append(observer)
    
    def notify(self, message: str):
        for observer in self._observers:
            observer.update(message)

# 4. Strategy Pattern
class SortStrategy(ABC):
    @abstractmethod
    def sort(self, data: List) -> List:
        pass

class QuickSort(SortStrategy):
    def sort(self, data: List) -> List:
        if len(data) <= 1:
            return data
        pivot = data[len(data) // 2]
        left = [x for x in data if x < pivot]
        middle = [x for x in data if x == pivot]
        right = [x for x in data if x > pivot]
        return self.sort(left) + middle + self.sort(right)

class DataSorter:
    def __init__(self, strategy: SortStrategy):
        self.strategy = strategy
    
    def sort(self, data: List) -> List:
        return self.strategy.sort(data)

# Usage examples
s1 = Singleton()
s2 = Singleton()
print(f"Singleton: same instance? {s1 is s2}")

animal = AnimalFactory.create_animal("dog")
print(f"Factory: {animal.speak()}")

sorter = DataSorter(QuickSort())
sorted_data = sorter.sort([3, 1, 4, 1, 5, 9, 2, 6])
print(f"Strategy: {sorted_data}")
#+end_src

* Resources and Next Steps
:PROPERTIES:
:CUSTOM_ID: sec:resources
:END:

** Essential Libraries Summary

| Category              | Libraries                              | Use Case                          |
|-----------------------+----------------------------------------+-----------------------------------|
| Numerical Computing   | NumPy, SciPy                          | Arrays, linear algebra, calculus  |
| Data Analysis         | Pandas                                | DataFrames, time series           |
| Visualization         | Matplotlib, Seaborn, Plotly           | Static and interactive plots      |
| Machine Learning      | Scikit-learn                          | Classical ML algorithms           |
| Deep Learning         | PyTorch, TensorFlow/Keras             | Neural networks, GPUs             |
| Image Processing      | OpenCV, Pillow, scikit-image          | Computer vision                   |
| Natural Language      | NLTK, spaCy, Transformers             | NLP, text processing              |
| Web Frameworks        | FastAPI, Flask, Django                | APIs and web apps                 |
| Async/Concurrency     | asyncio, multiprocessing, joblib      | Parallel computing                |
| Testing               | pytest, unittest, hypothesis          | Unit and integration tests        |
| Type Checking         | mypy, pyright                         | Static type analysis              |
| Code Quality          | black, flake8, pylint, ruff           | Formatting and linting            |

** Learning Path Recommendations

1. **Fundamentals** (Weeks 1-2)
   - Master Python syntax and data structures
   - Practice with LeetCode, HackerRank
   - Learn list/dict comprehensions, generators

2. **NumPy & Pandas** (Week 3)
   - Work with real datasets from Kaggle
   - Practice array operations and data cleaning

3. **Visualization** (Week 4)
   - Create various plot types
   - Build dashboards with Plotly

4. **Machine Learning** (Weeks 5-6)
   - Scikit-learn tutorials
   - Implement algorithms from scratch
   - Kaggle competitions

5. **Deep Learning** (Weeks 7-10)
   - PyTorch/TensorFlow tutorials
   - Build CNNs, RNNs, Transformers
   - Fine-tune pre-trained models

6. **Advanced Topics** (Ongoing)
   - Contribute to open-source projects
   - Read research papers
   - Build portfolio projects

** Online Resources

#+begin_quote
- *Documentation*: python.org, numpy.org, scikit-learn.org
- *Courses*: fast.ai, Coursera Deep Learning, MIT OpenCourseWare
- *Books*: "Python for Data Analysis" (McKinney), "Hands-On Machine Learning" (Géron)
- *Communities*: Stack Overflow, Reddit r/Python, r/MachineLearning
- *Practice*: Kaggle, LeetCode, Project Euler
#+end_quote

* Conclusion
:PROPERTIES:
:CUSTOM_ID: sec:final_conclusion
:END:

This guide covered comprehensive Python programming from fundamentals to advanced scientific computing. You've learned:

- **Core Python**: Data structures, functions, OOP, async programming
- **NumPy**: Array operations, broadcasting, linear algebra
- **SciPy**: Optimization, integration, statistical analysis
- **Pandas**: Data manipulation, cleaning, time series
- **Visualization**: Matplotlib, Seaborn for beautiful plots
- **Machine Learning**: Scikit-learn for classical ML
- **Deep Learning**: PyTorch and TensorFlow for neural networks
- **Best Practices**: Code style, testing, performance optimization

The key to mastery is consistent practice and building real projects. Start with small scripts, progress to data analysis projects, and eventually tackle machine learning applications.

#+begin_src python
"""Thank you for following this comprehensive guide!"""
print("=" * 60)
print("  Python Programming & Scientific Computing Guide Complete")
print("=" * 60)
print("\nKey takeaways:")
print("  • Python 3.13 brings significant performance improvements")
print("  • NumPy/Pandas are essential for data science")
print("  • Scikit-learn covers classical machine learning")
print("  • PyTorch/TensorFlow for deep learning")
print("  • Practice, practice, practice!")
print("\nHappy coding! 🐍🚀")
#+end_src

* Local Variables :noexport:

#+begin_src emacs-lisp :exports none :results silent
;; Ensure figures directory
(make-directory "./figures" t)

;; Auto-refresh images
(add-hook 'org-babel-after-execute-hook 'org-redisplay-inline-images)
#+end_src
